A deep dive into LeNet5.
The 1998 paper, Gradient-Based Learning Applied to Document Recognition, introduced LeNet5, a convolutional network designed for handwritten character recognition.
At that time, most recognition systems relied on handcrafted features, such as edge detectors or histograms.
These approaches required domain expertise and often struggled with the wide variability
in handwriting styles.
What struck me while reading the paper is how bold it was to move away from feature engineering and let the network learn everything directly from pixels.
LeNet5 was trained on the MNIST dataset of 28x28 grayscale digits.
The author padded the images to 32x32 and trained the model with backpropagation, reaching around 98.9% test accuracy, very high for the time.
Even though the dataset is simply by today's standards,
I still remember using MNIST in my own coursework and realizing how quickly CNNs outperformed traditional methods.
What makes LeanNet particularly important is it introduced building blocks that are still used today.
Local receptive fields, weight sharing, pooling, and multi-layer hierarchical feature extraction.
Even in my own classes on neural networks, this was the model we studied when we first transitioned from fully connected architectures to convolutional ones.
Looking back now, after working with larger CNNs in internships, it's fascinating to see how many of the core ideas were already present in 1998.
If Leenet was the spark, AlexNet was the explosion.
Published in 2012 by Alex Krasinski, Ilya Suskiver and Jeffrey Hinton, AlexNet stunned the computer vision community by winning the ImageNet large-scale visual recognition challenge with a massive margin.
Its top 5 error was 15.3% compared to the runner-up at 26.2%.
That single result is often credited with kicking off the deep learning boom of the past decade.
What makes AlexNet significant is not just its accuracy but how it combined several ideas that had been floating around in the research community.
Larger datasets, GPUs for training, ReLU activations, and dropout regularizations were all put together in one coherent model.
that showed the world deep learning could also scale.
Even in my own coursework, AlexNet came right after LeanNet in the timeline.
It was the example of how scaling up data, compute and network size could turn a neat idea into a dominant paradigm.
Revisiting it now, after having worked on modern CNNs, I can see clearly how many techniques we take for granted today were popularized by this paper.
The architecture of AlexNet looks simple by today's standards, but it was massive in 2012.
It had about 60 million parameters and was trained on two GPUs in parallel.
When I trained this on a subset of ImageNet, the model quickly became computationally heavy compared to LeanNet or even smaller CNNs.
It made me appreciate just how critical GPUs were for the success of AlexNet.
Without them, the training time would have been completely unrealistic.
What I learned was that AlexNet's main contribution wasn't inventing CNNs, but proving they could scale and outperform traditional methods on a large, complex dataset.
What surprised me?
How modern practices were already in place by 2012, like ReLU, Dropout, and Augmentation.
What I'm curious about is the split across two GPUs was done manually.
I would like to dive deeper into how they parallelized, layered and whether similar tricks could be relevant today with multi GPU training.
Connection to later models, AlexNet directly inspired VGG and ResNet.
Each one made the network deeper and more efficient, but the template was set here.
Convolutional blocks, plus pooling, plus fully connected layers.
After AlexNet shook the field in 2012, researchers quickly began exploring whether deeper networks could perform even better.
In 2014, Karen Simoyan and Andrew Zinserman introduced the VGG networks from the University of Oxford's Visual Geometry Group.
The main insight was that increasing depth with smaller filters could dramatically improve performance on ImageNet.
Unlike AlexNet, which mixed large convolution filters with varying kernel sizes, VGG used a very simple and consistent design.
stacks of 3x3 convolutions and 2x2 max pooling layers.
This uniformity made the network easier to understand and extend, and it showed that going deeper with smaller kernels was the way forward.
When I first learned about VGG in class, it was presented as the clean architecture, almost boring compared to GoogleNet or ResNet, but that's what made it so influential.
The simplicity was its strength.
Training VGG was much more computationally expensive than AlexNet.
It highlighted the need for faster GPUs and more memory.
Despite that, it became hugely popular in research, especially for transfer learning or smaller datasets.
What I learned was that VGG showed that architecture designs doesn't always need to be clever or complex.
Depth and consistency can be enough.
What surprised me was that how many modern vision tasks still rely on VGG features as a backbone, even though the model is over a decade old.
What I'm curious about, VGG had a huge number of parameters due to the fully connected layers.
Later models dropped them for efficiency.
I would like to explore how much performance really depends on those fully connected layers.
Connection to later models, VGG directly influenced Dressnet, which took the idea of depth even further but solved the optimization issues that can come with it.
By 2015, the trend in computer vision was clear.
Deeper networks kept winning.
AlexNet had 8 layers, VGG pushed it to 16 or 19 layers, and GoogleNet went even deeper with inception modules.
But there was a growing issue.
Many networks deeper did not always make them better.
In fact, researchers found that simply stacking more layers often led to worse performance due to the vanishing gradient problem.
ResNet, proposed by Kaiming, he and colleagues at Microsoft Research, solved this with a deceptively simple idea.
Residual connections.
Instead of forcing each layer to learn a fully transformation
Instead of forcing each layer to learn a full transformation, ResNet allowed layers to learn residuals, small changes relative to the input.
This made optimization dramatically easier and enabled training of networks with over 100 layers.
When I first read about ResNet, it stood out because of how minimal the change was, yet how big the impact turned out to be.
A single skip connection redefined the way deep networks were built.
What struck me when coding this was how natural the skip connection feels.
Adding the input back to the output doesn't add much complexity but changes training dynamics completely.
When I trained a small ResNet on CIFAR-10, it converged more reliably than a plain deep CNN of similar size.
What I learned, ResNet solved the degradation problem in deep networks and opened the door for extremely deep architectures.
What surprised me, the skip connection is such a small modification but it became a fundamental building block across deep learning, not just in vision.
What I'm curious about, later papers built on this with DenseNet, dense connections and transformers which is residual plus attention.
I would like to explore how the skip connection idea generalizes across architectures.
Connections to later models.
ResNet influenced almost every model that came after, from detection to segmentation to vision transformers.
Residual connections are everywhere.
By 2014, deeper models were clearly performing better, but there was a major problem.
The number of parameters was exploding.
VGG-16 had around 138 million parameters, making it expensive to train and deploy.
Researchers at Google introduced GoogleNet, which is also called Inception version 1.
as a way to go deeper without dramatically increasing computation.
The key idea was the inception module, which allowed the network to look at multiple receptive field sizes at the same time and concatenate the results.
This captured information at different scales while keeping efficiency under control.
Another important trick was the use of 1x1 convolutions for dimensionality reduction, before the heavier 3x3 and 5x5 convolutions.
When I first saw GoogleNet in class, it looked very different from the straightforward stack of Leos and AlexNet or VGG.
It was the first time I realized that CNNstint always need to be linear chains of Leos.
You could design smarter modules and reuse them.
GoogleNet was 22 layers deep but had only 5 million parameters which was far less than VGG.
It won the ImageNet 2014 challenge with a top 5 error of 6.7%.
This modular design made it possible to build very deep networks without having to blow up the number of parameters.
When I tried coding it, the inception module felt surprisingly elegant.
Each branch is simple, but the concatenation at the end gives it richness.
What I learned, GoogleNet showed that depth alone wasn't enough.
Efficiency and smart design choices mattered just as much.
What surprised me, fully connected layers were almost completely removed.
Global average pooling was a new idea at the time and became a standard.
What I'm curious about, the auxiliary classifiers are rarely used today.
I would like to explore why they helped in GoogleNet and why they didn't stick around in later modules.
connection to later models.
The inception idea evolved the inception into V2 or V3 and eventually inspired architectures like ResNet and Exception.
It marked a transition from linear stacking to modular design.
Vision transformers have recently emerged as a competitive alternative to convolutional neural networks that are currently state-of-the-art in different image recognition and computer vision tasks.
VIT models outperform the current SOTA CNNs by almost four times in terms of computational efficiency and accuracy.
Transformer models have become the de facto status quo in natural language processing.
For example, the popular ChatGPT AI chatbot is a transformer-based language model.
Specifically, it is based on the GPT architecture.
This uses self-attention mechanisms to model the dependencies between words in a text.
While the transformer architecture has become the highest standard for tasks involving natural language processing, its use cases relating to computer vision remain limited.
In many computer vision tasks, we use attention either in conjunction with the convolutional neural networks or to substitute certain aspects of the CNNs while maintaining their entire composition.
Popular image recognition models include Residual Networks, VGG, YOLOv3 and Segment Anything.
However, this dependency on CNN is not mandatory and a pure transformer applied directly to sequences of image patches can work exceptionally well on image classification tasks.
Vision transformers have recently achieved high competitive performance in benchmarks for several computer vision applications such as image classification, object detection, and semantic image segmentation.
Transformer is an efficient and effective transformer-based backbone for general purpose vision tasks.
It uses a new technique called cross-shaped window self-attention to analyze different parts of the image simultaneously, making it much faster.
The CSWin transformer has surpassed previous SOTA methods like the Swin transformer.
In benchmark tasks, CSWin achieved excellent performance including 85.4% top 1 accuracy on ImageNet.
The Vision Transformer model architecture was introduced in a research paper published as a conference paper at ICLR 2021, titled An Image is Worth 16x16 Words, Transformers for Image Recognition at Scale.
It was developed and published by
10 more authors of Google Research brain team.
The fine-tuning code and pre-trained VIT models are available on the GitHub of the Google Research team.
You can find them here.
The VIT models were pre-trained on the ImageNet and ImageNet21k datasets.
Origin and History of Vision Transformer Models In the following, we highlight some of the most significant vision transformer developments over the years.
These developments are based on the transformer architecture originally proposed for natural language processing.
A transformer in machine learning is a deep learning model that uses the mechanisms of attention, differentially weighing the significance of each part of the input sequence of data.
Transformers in machine learning are composed of multiple self-attention layers.
They are primarily used in the AI subfields of natural language processing.
Transformation in machine learning holds strong promises towards a generic learning method that can be applied to various data modalities, including the recent breakthroughs in computer vision achieving standard accuracy with better parameter efficiency.
Image classification is a fundamental task in computer vision that involves assigning a label to an image based on its content
Over the years, deep CNNs like YOLOv7 have been the state-of-the-art method for image classification.
