gitansh_0.wav|A deep dive into LeNet5.|1
gitansh_1.wav|The 1998 paper, Gradient-Based Learning Applied to Document Recognition, introduced LeNet5, a convolutional network designed for handwritten character recognition.|1
gitansh_2.wav|At that time, most recognition systems relied on handcrafted features, such as edge detectors or histograms.|1
gitansh_3.wav|These approaches required domain expertise and often struggled with the wide variability|1
gitansh_4.wav|in handwriting styles.|1
gitansh_6.wav|LeNet5 was trained on the MNIST dataset of 28x28 grayscale digits.|1
gitansh_8.wav|Even though the dataset is simply by today's standards,|1
gitansh_9.wav|I still remember using MNIST in my own coursework and realizing how quickly CNNs outperformed traditional methods.|1
gitansh_10.wav|What makes LeanNet particularly important is it introduced building blocks that are still used today.|1
gitansh_11.wav|Local receptive fields, weight sharing, pooling, and multi-layer hierarchical feature extraction.|1
gitansh_12.wav|Even in my own classes on neural networks, this was the model we studied when we first transitioned from fully connected architectures to convolutional ones.|1
gitansh_13.wav|Looking back now, after working with larger CNNs in internships, it's fascinating to see how many of the core ideas were already present in 1998.|1
gitansh_14.wav|If Leenet was the spark, AlexNet was the explosion.|1
gitansh_16.wav|Its top 5 error was 15.3% compared to the runner-up at 26.2%.|1
gitansh_17.wav|That single result is often credited with kicking off the deep learning boom of the past decade.|1
gitansh_18.wav|What makes AlexNet significant is not just its accuracy but how it combined several ideas that had been floating around in the research community.|1
gitansh_19.wav|Larger datasets, GPUs for training, ReLU activations, and dropout regularizations were all put together in one coherent model.|1
gitansh_20.wav|that showed the world deep learning could also scale.|1
gitansh_21.wav|Even in my own coursework, AlexNet came right after LeanNet in the timeline.|1
gitansh_22.wav|It was the example of how scaling up data, compute and network size could turn a neat idea into a dominant paradigm.|1
gitansh_23.wav|Revisiting it now, after having worked on modern CNNs, I can see clearly how many techniques we take for granted today were popularized by this paper.|1
gitansh_24.wav|The architecture of AlexNet looks simple by today's standards, but it was massive in 2012.|1
gitansh_25.wav|It had about 60 million parameters and was trained on two GPUs in parallel.|1
gitansh_26.wav|When I trained this on a subset of ImageNet, the model quickly became computationally heavy compared to LeanNet or even smaller CNNs.|1
gitansh_27.wav|It made me appreciate just how critical GPUs were for the success of AlexNet.|1
gitansh_28.wav|Without them, the training time would have been completely unrealistic.|1
gitansh_29.wav|What I learned was that AlexNet's main contribution wasn't inventing CNNs, but proving they could scale and outperform traditional methods on a large, complex dataset.|1
gitansh_31.wav|How modern practices were already in place by 2012, like ReLU, Dropout, and Augmentation.|1
gitansh_32.wav|What I'm curious about is the split across two GPUs was done manually.|1
gitansh_33.wav|I would like to dive deeper into how they parallelized, layered and whether similar tricks could be relevant today with multi GPU training.|1
gitansh_34.wav|Connection to later models, AlexNet directly inspired VGG and ResNet.|1
gitansh_35.wav|Each one made the network deeper and more efficient, but the template was set here.|1
gitansh_36.wav|Convolutional blocks, plus pooling, plus fully connected layers.|1
gitansh_37.wav|After AlexNet shook the field in 2012, researchers quickly began exploring whether deeper networks could perform even better.|1
gitansh_38.wav|In 2014, Karen Simoyan and Andrew Zinserman introduced the VGG networks from the University of Oxford's Visual Geometry Group.|1
gitansh_39.wav|The main insight was that increasing depth with smaller filters could dramatically improve performance on ImageNet.|1
gitansh_40.wav|Unlike AlexNet, which mixed large convolution filters with varying kernel sizes, VGG used a very simple and consistent design.|1
gitansh_41.wav|stacks of 3x3 convolutions and 2x2 max pooling layers.|1
gitansh_42.wav|This uniformity made the network easier to understand and extend, and it showed that going deeper with smaller kernels was the way forward.|1
gitansh_43.wav|When I first learned about VGG in class, it was presented as the clean architecture, almost boring compared to GoogleNet or ResNet, but that's what made it so influential.|1
gitansh_44.wav|The simplicity was its strength.|1
gitansh_45.wav|Training VGG was much more computationally expensive than AlexNet.|1
gitansh_46.wav|It highlighted the need for faster GPUs and more memory.|1
gitansh_47.wav|Despite that, it became hugely popular in research, especially for transfer learning or smaller datasets.|1
gitansh_48.wav|What I learned was that VGG showed that architecture designs doesn't always need to be clever or complex.|1
gitansh_49.wav|Depth and consistency can be enough.|1
gitansh_50.wav|What surprised me was that how many modern vision tasks still rely on VGG features as a backbone, even though the model is over a decade old.|1
gitansh_51.wav|What I'm curious about, VGG had a huge number of parameters due to the fully connected layers.|1
gitansh_52.wav|Later models dropped them for efficiency.|1
gitansh_53.wav|I would like to explore how much performance really depends on those fully connected layers.|1
gitansh_54.wav|Connection to later models, VGG directly influenced Dressnet, which took the idea of depth even further but solved the optimization issues that can come with it.|1
gitansh_55.wav|By 2015, the trend in computer vision was clear.|1
gitansh_57.wav|AlexNet had 8 layers, VGG pushed it to 16 or 19 layers, and GoogleNet went even deeper with inception modules.|1
gitansh_58.wav|But there was a growing issue.|1
gitansh_59.wav|Many networks deeper did not always make them better.|1
gitansh_60.wav|In fact, researchers found that simply stacking more layers often led to worse performance due to the vanishing gradient problem.|1
gitansh_61.wav|ResNet, proposed by Kaiming, he and colleagues at Microsoft Research, solved this with a deceptively simple idea.|1
gitansh_62.wav|Residual connections.|1
gitansh_63.wav|Instead of forcing each layer to learn a fully transformation|1
gitansh_64.wav|Instead of forcing each layer to learn a full transformation, ResNet allowed layers to learn residuals, small changes relative to the input.|1
gitansh_65.wav|This made optimization dramatically easier and enabled training of networks with over 100 layers.|1
gitansh_66.wav|When I first read about ResNet, it stood out because of how minimal the change was, yet how big the impact turned out to be.|1
gitansh_67.wav|A single skip connection redefined the way deep networks were built.|1
gitansh_68.wav|What struck me when coding this was how natural the skip connection feels.|1
gitansh_69.wav|Adding the input back to the output doesn't add much complexity but changes training dynamics completely.|1
gitansh_70.wav|When I trained a small ResNet on CIFAR-10, it converged more reliably than a plain deep CNN of similar size.|1
gitansh_71.wav|What I learned, ResNet solved the degradation problem in deep networks and opened the door for extremely deep architectures.|1
gitansh_72.wav|What surprised me, the skip connection is such a small modification but it became a fundamental building block across deep learning, not just in vision.|1
gitansh_73.wav|What I'm curious about, later papers built on this with DenseNet, dense connections and transformers which is residual plus attention.|1
gitansh_74.wav|I would like to explore how the skip connection idea generalizes across architectures.|1
gitansh_75.wav|Connections to later models.|1
gitansh_76.wav|ResNet influenced almost every model that came after, from detection to segmentation to vision transformers.|1
gitansh_77.wav|Residual connections are everywhere.|1
gitansh_78.wav|By 2014, deeper models were clearly performing better, but there was a major problem.|1
gitansh_79.wav|The number of parameters was exploding.|1
gitansh_80.wav|VGG-16 had around 138 million parameters, making it expensive to train and deploy.|1
gitansh_81.wav|Researchers at Google introduced GoogleNet, which is also called Inception version 1.|1
gitansh_82.wav|as a way to go deeper without dramatically increasing computation.|1
gitansh_83.wav|The key idea was the inception module, which allowed the network to look at multiple receptive field sizes at the same time and concatenate the results.|1
gitansh_84.wav|This captured information at different scales while keeping efficiency under control.|1
gitansh_86.wav|When I first saw GoogleNet in class, it looked very different from the straightforward stack of Leos and AlexNet or VGG.|1
gitansh_87.wav|It was the first time I realized that CNNstint always need to be linear chains of Leos.|1
gitansh_88.wav|You could design smarter modules and reuse them.|1
gitansh_89.wav|GoogleNet was 22 layers deep but had only 5 million parameters which was far less than VGG.|1
gitansh_90.wav|It won the ImageNet 2014 challenge with a top 5 error of 6.7%.|1
gitansh_91.wav|This modular design made it possible to build very deep networks without having to blow up the number of parameters.|1
gitansh_92.wav|When I tried coding it, the inception module felt surprisingly elegant.|1
gitansh_93.wav|Each branch is simple, but the concatenation at the end gives it richness.|1
gitansh_94.wav|What I learned, GoogleNet showed that depth alone wasn't enough.|1
gitansh_95.wav|Efficiency and smart design choices mattered just as much.|1
gitansh_96.wav|What surprised me, fully connected layers were almost completely removed.|1
gitansh_97.wav|Global average pooling was a new idea at the time and became a standard.|1
gitansh_98.wav|What I'm curious about, the auxiliary classifiers are rarely used today.|1
gitansh_99.wav|I would like to explore why they helped in GoogleNet and why they didn't stick around in later modules.|1
gitansh_100.wav|connection to later models.|1
gitansh_101.wav|The inception idea evolved the inception into V2 or V3 and eventually inspired architectures like ResNet and Exception.|1
gitansh_102.wav|It marked a transition from linear stacking to modular design.|1
gitansh_104.wav|VIT models outperform the current SOTA CNNs by almost four times in terms of computational efficiency and accuracy.|1
gitansh_105.wav|Transformer models have become the de facto status quo in natural language processing.|1
gitansh_106.wav|For example, the popular ChatGPT AI chatbot is a transformer-based language model.|1
gitansh_107.wav|Specifically, it is based on the GPT architecture.|1
gitansh_108.wav|This uses self-attention mechanisms to model the dependencies between words in a text.|1
gitansh_109.wav|While the transformer architecture has become the highest standard for tasks involving natural language processing, its use cases relating to computer vision remain limited.|1
gitansh_111.wav|Popular image recognition models include Residual Networks, VGG, YOLOv3 and Segment Anything.|1
gitansh_114.wav|Transformer is an efficient and effective transformer-based backbone for general purpose vision tasks.|1
gitansh_115.wav|It uses a new technique called cross-shaped window self-attention to analyze different parts of the image simultaneously, making it much faster.|1
gitansh_116.wav|The CSWin transformer has surpassed previous SOTA methods like the Swin transformer.|1
gitansh_117.wav|In benchmark tasks, CSWin achieved excellent performance including 85.4% top 1 accuracy on ImageNet.|1
gitansh_119.wav|It was developed and published by|1
gitansh_120.wav|10 more authors of Google Research brain team.|1
gitansh_121.wav|The fine-tuning code and pre-trained VIT models are available on the GitHub of the Google Research team.|1
gitansh_123.wav|The VIT models were pre-trained on the ImageNet and ImageNet21k datasets.|1
gitansh_124.wav|Origin and History of Vision Transformer Models In the following, we highlight some of the most significant vision transformer developments over the years.|1
gitansh_125.wav|These developments are based on the transformer architecture originally proposed for natural language processing.|1
gitansh_126.wav|A transformer in machine learning is a deep learning model that uses the mechanisms of attention, differentially weighing the significance of each part of the input sequence of data.|1
gitansh_127.wav|Transformers in machine learning are composed of multiple self-attention layers.|1
gitansh_128.wav|They are primarily used in the AI subfields of natural language processing.|1
gitansh_130.wav|Image classification is a fundamental task in computer vision that involves assigning a label to an image based on its content|1
gitansh_131.wav|Over the years, deep CNNs like YOLOv7 have been the state-of-the-art method for image classification.|1
gitansh_0.wav|A deep dive into LeNet5.|1
gitansh_1.wav|The 1998 paper, Gradient-Based Learning Applied to Document Recognition, introduced LeNet5, a convolutional network designed for handwritten character recognition.|1
gitansh_2.wav|At that time, most recognition systems relied on handcrafted features, such as edge detectors or histograms.|1
gitansh_3.wav|These approaches required domain expertise and often struggled with the wide variability|1
gitansh_4.wav|in handwriting styles.|1
gitansh_6.wav|LeNet5 was trained on the MNIST dataset of 28x28 grayscale digits.|1
gitansh_8.wav|Even though the dataset is simply by today's standards,|1
gitansh_9.wav|I still remember using MNIST in my own coursework and realizing how quickly CNNs outperformed traditional methods.|1
gitansh_10.wav|What makes LeanNet particularly important is it introduced building blocks that are still used today.|1
gitansh_11.wav|Local receptive fields, weight sharing, pooling, and multi-layer hierarchical feature extraction.|1
gitansh_12.wav|Even in my own classes on neural networks, this was the model we studied when we first transitioned from fully connected architectures to convolutional ones.|1
gitansh_13.wav|Looking back now, after working with larger CNNs in internships, it's fascinating to see how many of the core ideas were already present in 1998.|1
gitansh_14.wav|If Leenet was the spark, AlexNet was the explosion.|1
gitansh_16.wav|Its top 5 error was 15.3% compared to the runner-up at 26.2%.|1
gitansh_17.wav|That single result is often credited with kicking off the deep learning boom of the past decade.|1
gitansh_18.wav|What makes AlexNet significant is not just its accuracy but how it combined several ideas that had been floating around in the research community.|1
gitansh_19.wav|Larger datasets, GPUs for training, ReLU activations, and dropout regularizations were all put together in one coherent model.|1
gitansh_20.wav|that showed the world deep learning could also scale.|1
gitansh_21.wav|Even in my own coursework, AlexNet came right after LeanNet in the timeline.|1
gitansh_22.wav|It was the example of how scaling up data, compute and network size could turn a neat idea into a dominant paradigm.|1
gitansh_23.wav|Revisiting it now, after having worked on modern CNNs, I can see clearly how many techniques we take for granted today were popularized by this paper.|1
gitansh_24.wav|The architecture of AlexNet looks simple by today's standards, but it was massive in 2012.|1
gitansh_25.wav|It had about 60 million parameters and was trained on two GPUs in parallel.|1
gitansh_26.wav|When I trained this on a subset of ImageNet, the model quickly became computationally heavy compared to LeanNet or even smaller CNNs.|1
gitansh_27.wav|It made me appreciate just how critical GPUs were for the success of AlexNet.|1
gitansh_28.wav|Without them, the training time would have been completely unrealistic.|1
gitansh_29.wav|What I learned was that AlexNet's main contribution wasn't inventing CNNs, but proving they could scale and outperform traditional methods on a large, complex dataset.|1
gitansh_31.wav|How modern practices were already in place by 2012, like ReLU, Dropout, and Augmentation.|1
gitansh_32.wav|What I'm curious about is the split across two GPUs was done manually.|1
gitansh_33.wav|I would like to dive deeper into how they parallelized, layered and whether similar tricks could be relevant today with multi GPU training.|1
gitansh_34.wav|Connection to later models, AlexNet directly inspired VGG and ResNet.|1
gitansh_35.wav|Each one made the network deeper and more efficient, but the template was set here.|1
gitansh_36.wav|Convolutional blocks, plus pooling, plus fully connected layers.|1
gitansh_37.wav|After AlexNet shook the field in 2012, researchers quickly began exploring whether deeper networks could perform even better.|1
gitansh_38.wav|In 2014, Karen Simoyan and Andrew Zinserman introduced the VGG networks from the University of Oxford's Visual Geometry Group.|1
gitansh_39.wav|The main insight was that increasing depth with smaller filters could dramatically improve performance on ImageNet.|1
gitansh_40.wav|Unlike AlexNet, which mixed large convolution filters with varying kernel sizes, VGG used a very simple and consistent design.|1
gitansh_41.wav|stacks of 3x3 convolutions and 2x2 max pooling layers.|1
gitansh_42.wav|This uniformity made the network easier to understand and extend, and it showed that going deeper with smaller kernels was the way forward.|1
gitansh_43.wav|When I first learned about VGG in class, it was presented as the clean architecture, almost boring compared to GoogleNet or ResNet, but that's what made it so influential.|1
gitansh_44.wav|The simplicity was its strength.|1
gitansh_45.wav|Training VGG was much more computationally expensive than AlexNet.|1
gitansh_46.wav|It highlighted the need for faster GPUs and more memory.|1
gitansh_47.wav|Despite that, it became hugely popular in research, especially for transfer learning or smaller datasets.|1
gitansh_48.wav|What I learned was that VGG showed that architecture designs doesn't always need to be clever or complex.|1
gitansh_49.wav|Depth and consistency can be enough.|1
gitansh_50.wav|What surprised me was that how many modern vision tasks still rely on VGG features as a backbone, even though the model is over a decade old.|1
gitansh_51.wav|What I'm curious about, VGG had a huge number of parameters due to the fully connected layers.|1
gitansh_52.wav|Later models dropped them for efficiency.|1
gitansh_53.wav|I would like to explore how much performance really depends on those fully connected layers.|1
gitansh_54.wav|Connection to later models, VGG directly influenced Dressnet, which took the idea of depth even further but solved the optimization issues that can come with it.|1
gitansh_55.wav|By 2015, the trend in computer vision was clear.|1
gitansh_57.wav|AlexNet had 8 layers, VGG pushed it to 16 or 19 layers, and GoogleNet went even deeper with inception modules.|1
gitansh_58.wav|But there was a growing issue.|1
gitansh_59.wav|Many networks deeper did not always make them better.|1
gitansh_60.wav|In fact, researchers found that simply stacking more layers often led to worse performance due to the vanishing gradient problem.|1
gitansh_61.wav|ResNet, proposed by Kaiming, he and colleagues at Microsoft Research, solved this with a deceptively simple idea.|1
gitansh_62.wav|Residual connections.|1
gitansh_63.wav|Instead of forcing each layer to learn a fully transformation|1
gitansh_64.wav|Instead of forcing each layer to learn a full transformation, ResNet allowed layers to learn residuals, small changes relative to the input.|1
gitansh_65.wav|This made optimization dramatically easier and enabled training of networks with over 100 layers.|1
gitansh_66.wav|When I first read about ResNet, it stood out because of how minimal the change was, yet how big the impact turned out to be.|1
gitansh_67.wav|A single skip connection redefined the way deep networks were built.|1
gitansh_68.wav|What struck me when coding this was how natural the skip connection feels.|1
gitansh_69.wav|Adding the input back to the output doesn't add much complexity but changes training dynamics completely.|1
gitansh_70.wav|When I trained a small ResNet on CIFAR-10, it converged more reliably than a plain deep CNN of similar size.|1
gitansh_71.wav|What I learned, ResNet solved the degradation problem in deep networks and opened the door for extremely deep architectures.|1
gitansh_72.wav|What surprised me, the skip connection is such a small modification but it became a fundamental building block across deep learning, not just in vision.|1
gitansh_73.wav|What I'm curious about, later papers built on this with DenseNet, dense connections and transformers which is residual plus attention.|1
gitansh_74.wav|I would like to explore how the skip connection idea generalizes across architectures.|1
gitansh_75.wav|Connections to later models.|1
gitansh_76.wav|ResNet influenced almost every model that came after, from detection to segmentation to vision transformers.|1
gitansh_77.wav|Residual connections are everywhere.|1
gitansh_78.wav|By 2014, deeper models were clearly performing better, but there was a major problem.|1
gitansh_79.wav|The number of parameters was exploding.|1
gitansh_80.wav|VGG-16 had around 138 million parameters, making it expensive to train and deploy.|1
gitansh_81.wav|Researchers at Google introduced GoogleNet, which is also called Inception version 1.|1
gitansh_82.wav|as a way to go deeper without dramatically increasing computation.|1
gitansh_83.wav|The key idea was the inception module, which allowed the network to look at multiple receptive field sizes at the same time and concatenate the results.|1
gitansh_84.wav|This captured information at different scales while keeping efficiency under control.|1
gitansh_86.wav|When I first saw GoogleNet in class, it looked very different from the straightforward stack of Leos and AlexNet or VGG.|1
gitansh_87.wav|It was the first time I realized that CNNstint always need to be linear chains of Leos.|1
gitansh_88.wav|You could design smarter modules and reuse them.|1
gitansh_89.wav|GoogleNet was 22 layers deep but had only 5 million parameters which was far less than VGG.|1
gitansh_90.wav|It won the ImageNet 2014 challenge with a top 5 error of 6.7%.|1
gitansh_91.wav|This modular design made it possible to build very deep networks without having to blow up the number of parameters.|1
gitansh_92.wav|When I tried coding it, the inception module felt surprisingly elegant.|1
gitansh_93.wav|Each branch is simple, but the concatenation at the end gives it richness.|1
gitansh_94.wav|What I learned, GoogleNet showed that depth alone wasn't enough.|1
gitansh_95.wav|Efficiency and smart design choices mattered just as much.|1
gitansh_96.wav|What surprised me, fully connected layers were almost completely removed.|1
gitansh_97.wav|Global average pooling was a new idea at the time and became a standard.|1
gitansh_98.wav|What I'm curious about, the auxiliary classifiers are rarely used today.|1
gitansh_99.wav|I would like to explore why they helped in GoogleNet and why they didn't stick around in later modules.|1
gitansh_100.wav|connection to later models.|1
gitansh_101.wav|The inception idea evolved the inception into V2 or V3 and eventually inspired architectures like ResNet and Exception.|1
gitansh_102.wav|It marked a transition from linear stacking to modular design.|1
gitansh_104.wav|VIT models outperform the current SOTA CNNs by almost four times in terms of computational efficiency and accuracy.|1
gitansh_105.wav|Transformer models have become the de facto status quo in natural language processing.|1
gitansh_106.wav|For example, the popular ChatGPT AI chatbot is a transformer-based language model.|1
gitansh_107.wav|Specifically, it is based on the GPT architecture.|1
gitansh_108.wav|This uses self-attention mechanisms to model the dependencies between words in a text.|1
gitansh_109.wav|While the transformer architecture has become the highest standard for tasks involving natural language processing, its use cases relating to computer vision remain limited.|1
gitansh_111.wav|Popular image recognition models include Residual Networks, VGG, YOLOv3 and Segment Anything.|1
gitansh_114.wav|Transformer is an efficient and effective transformer-based backbone for general purpose vision tasks.|1
gitansh_115.wav|It uses a new technique called cross-shaped window self-attention to analyze different parts of the image simultaneously, making it much faster.|1
gitansh_116.wav|The CSWin transformer has surpassed previous SOTA methods like the Swin transformer.|1
gitansh_117.wav|In benchmark tasks, CSWin achieved excellent performance including 85.4% top 1 accuracy on ImageNet.|1
gitansh_119.wav|It was developed and published by|1
gitansh_120.wav|10 more authors of Google Research brain team.|1
gitansh_121.wav|The fine-tuning code and pre-trained VIT models are available on the GitHub of the Google Research team.|1
gitansh_123.wav|The VIT models were pre-trained on the ImageNet and ImageNet21k datasets.|1
gitansh_124.wav|Origin and History of Vision Transformer Models In the following, we highlight some of the most significant vision transformer developments over the years.|1
gitansh_125.wav|These developments are based on the transformer architecture originally proposed for natural language processing.|1
gitansh_126.wav|A transformer in machine learning is a deep learning model that uses the mechanisms of attention, differentially weighing the significance of each part of the input sequence of data.|1
gitansh_127.wav|Transformers in machine learning are composed of multiple self-attention layers.|1
gitansh_128.wav|They are primarily used in the AI subfields of natural language processing.|1
gitansh_130.wav|Image classification is a fundamental task in computer vision that involves assigning a label to an image based on its content|1
gitansh_131.wav|Over the years, deep CNNs like YOLOv7 have been the state-of-the-art method for image classification.|1
